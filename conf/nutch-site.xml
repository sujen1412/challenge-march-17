<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

<!-- general properties  -->

<property>
  <name>store.ip.address</name>
  <value>true</value>
  <description>Enables us to capture the specific IP address
  (InetSocketAddress) of the host which we connect to via
  the given protocol. Currently supported is protocol-ftp and
  http.
  </description>
</property>

<property>
  <name>http.agent.name</name>
  <value>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36</value>
  <description>HTTP 'User-Agent' request header. MUST NOT be empty -
  please set this to a single word uniquely related to your organization.

  NOTE: You should also check other related properties:

    http.robots.agents
    http.agent.description
    http.agent.url
    http.agent.email
    http.agent.version

  and set their values appropriately.

  </description>
</property>

<property>
  <name>http.robot.rules.whitelist</name>
  <value>finance.yahoo.com</value> <!-- Changed from *.ftfindustries.com -->
  <description>Comma separated list of hostnames or IP addresses to ignore 
  robot rules parsing for. Use with care and only if you are explicitly
  allowed by the site owner to ignore the site's robots.txt!
  </description>
</property>

<property>
  <name>http.agent.rotate</name>
  <value>true</value>
  <description>
    If true, instead of http.agent.name, alternating agent names are
    chosen from a list provided via http.agent.rotate.file.
  </description>
</property>

<property>
  <name>http.agent.rotate.file</name>
  <value>agents.txt</value>
  <description>
    File containing alternative user agent names to be used instead of
    http.agent.name on a rotating basis if http.agent.rotate is true.
    Each line of the file should contain exactly one agent
    specification including name, version, description, URL, etc.
  </description>
</property>

<property>
  <name>http.max.delays</name>
  <value>10</value> <!-- Changed from 25 -->
  <description>The number of times a thread will delay when trying to
  fetch a page.  Each time it finds that a host is busy, it will wait
  fetcher.server.delay.  After http.max.delays attepts, it will give
  up on the page for now.</description>
</property>

<property>
  <name>http.content.limit</name>
  <value>4194304</value>
  <description>The length limit for downloaded content using the http://
  protocol, in bytes. If this value is nonnegative (>=0), content longer
  than it will be truncated; otherwise, no truncation at all. Do not
  confuse this setting with the file.content.limit setting.
  </description>
</property>

<property>
  <name>http.verbose</name>
  <value>true</value>
  <description>If true, HTTP will log more verbosely.</description>
</property>

<property>
  <name>http.redirect.max</name>
  <value>5</value>
  <description>The maximum number of redirects the fetcher will follow when
  trying to fetch a page. If set to negative or 0, fetcher won't immediately
  follow redirected URLs, instead it will record them for later fetching.
  </description>
</property>

<property>
  <name>http.timeout</name>
  <value>60000</value> <!-- Changed from 10000 -->
  <description>The default network timeout, in milliseconds.</description>
</property>

<!-- FTP properties -->

<!-- web db properties -->

<property>
  <name>db.preserve.backup</name>
  <value>true</value>
  <description>If true, updatedb will keep a backup of the previous CrawlDB
  version in the old directory. In case of disaster, one can rename old to 
  current and restore the CrawlDB to its previous state.
  </description>
</property>

<property>
    <name>db.url.normalizers</name>
    <value>true</value>
    <description>Normalize urls when updating crawldb</description>
</property>

<property>
    <name>db.url.filters</name>
    <value>true</value>
    <description>Filter urls when updating crawldb</description>
</property>

<property>
  <name>db.ignore.internal.links</name>
  <value>false</value>
  <description>If true, when adding new links to a page, links from
  the same host are ignored.  This is an effective way to limit the
  size of the link database, keeping only the highest quality
  links.
  </description>
</property>

<property>
  <name>db.max.outlinks.per.page</name>
  <value>-1</value>
  <description>The maximum number of outlinks that we'll process for a page.
  If this value is nonnegative (>=0), at most db.max.outlinks.per.page outlinks
  will be processed for a page; otherwise, all outlinks will be processed.
  </description>
</property>

<property>
  <name>db.max.anchor.length</name>
  <value>1000</value>
  <description>The maximum number of characters permitted in an anchor.
  </description>
</property>

<property>
  <name>db.ignore.external.links</name>
  <value>false</value> <!-- Changed from false -->
  <description>If true, outlinks leading from a page to external hosts or domain
  will be ignored. This is an effective way to limit the crawl to include
  only initially injected hosts, without creating complex URLFilters.
  See 'db.ignore.external.links.mode'.
  </description>
</property>

<property>
  <name>db.update.purge.404</name>
  <value>false</value> <!-- Changed from false -->
  <description>If true, updatedb will add purge records with status DB_GONE
  from the CrawlDB.
  </description>
</property>

<property>
  <name>db.max.outlinks.per.page</name>
  <value>-1</value> <!-- Changed from 100 -->
  <description>The maximum number of outlinks that we'll process for a page.
  If this value is nonnegative (>=0), at most db.max.outlinks.per.page outlinks
  will be processed for a page; otherwise, all outlinks will be processed.
  </description>
</property>

<property>
  <name>db.signature.class</name>
  <value>org.apache.nutch.crawl.TextMD5Signature</value>
  <description>The default implementation of a page signature. Signatures
  created with this implementation will be used for duplicate detection
  and removal.</description>
</property>
<property>
  <name>linkdb.ignore.internal.links</name>
  <value>false</value>
  <description>If true, when adding new links to a page, links from
  the same host are ignored.  This is an effective way to limit the
  size of the link database, keeping only the highest quality
  links.
  </description>
</property>


<!-- generate properties -->

<property>
  <name>generate.max.count</name>
  <value>100</value>
  <description>The maximum number of urls in a single
  fetchlist.  -1 if unlimited. The urls are counted according
  to the value of the parameter generator.count.mode.
  </description>
</property>

<property>
  <name>generate.update.crawldb</name>
  <value>true</value>
  <description>For highly-concurrent environments, where several
  generate/fetch/update cycles may overlap, setting this to true ensures
  that generate will create different fetchlists even without intervening
  updatedb-s, at the cost of running an additional job to update CrawlDB.
  If false, running generate twice without intervening
  updatedb will generate identical fetchlists.</description>
</property>

<!-- urlpartitioner properties -->

<!-- fetcher properties -->

<property>
  <name>fetcher.server.min.delay</name>
  <value>1</value> <!-- Changed from 6.0 -->
  <description>The minimum number of seconds the fetcher will delay between 
  successive requests to the same server. This value is applicable ONLY
  if fetcher.threads.per.queue is greater than 1 (i.e. the host blocking
  is turned off).</description>
</property>

<property>
 <name>fetcher.max.crawl.delay</name>
 <value>30</value><!-- 10 -->
 <description>
 If the Crawl-Delay in robots.txt is set to greater than this value (in
 seconds) then the fetcher will skip this page, generating an error report.
 If set to -1 the fetcher will never skip such pages and will wait the
 amount of time retrieved from robots.txt Crawl-Delay, however long that
 might be.
 </description>
</property> 

<property>
  <name>fetcher.threads.fetch</name>
  <value>20</value><!-- 10 -->
  <description>The number of FetcherThreads the fetcher should use.
  This is also determines the maximum number of requests that are
  made at once (each FetcherThread handles one connection). The total
  number of threads running in distributed mode will be the number of
  fetcher threads * number of nodes as fetcher has one map task per node.
  </description>
</property>

<property>
  <name>fetcher.threads.per.queue</name>
  <value>50</value><!-- 1 -->
  <description>This number is the maximum number of threads that
    should be allowed to access a queue at one time. Setting it to 
    a value > 1 will cause the Crawl-Delay value from robots.txt to
    be ignored and the value of fetcher.server.min.delay to be used
    as a delay between successive requests to the same server instead 
    of fetcher.server.delay.
   </description>
</property>

<property>
  <name>fetcher.verbose</name>
  <value>true</value>
  <description>If true, fetcher will log more verbosely.</description>
</property>

<property>
  <name>fetcher.parse</name>
  <value>true</value> <!-- Changed from false -->
  <description>If true, fetcher will parse content. Default is false, which means
  that a separate parsing step is required after fetching is finished.</description>
</property>

<property>
  <name>fetcher.store.content</name>
  <value>true</value>
  <description>If true, fetcher will store content.</description>
</property>

<property>
  <name>fetcher.timelimit.mins</name>
  <value>-1</value>
  <description>This is the number of minutes allocated to the fetching.
  Once this value is reached, any remaining entry from the input URL list is skipped 
  and all active queues are emptied. The default value of -1 deactivates the time limit.
  </description>
</property>

<property>
  <name>fetcher.max.exceptions.per.queue</name>
  <value>-1</value><!-- -1 -->
  <description>The maximum number of protocol-level exceptions (e.g. timeouts) per
  host (or IP) queue. Once this value is reached, any remaining entries from this
  queue are purged, effectively stopping the fetching from this host/IP. The default
  value of -1 deactivates this limit.
  </description>
</property>

<property>
  <name>fetcher.follow.outlinks.depth</name>
  <value>1</value>
  <description>(EXPERT)When fetcher.parse is true and this value is greater than 0 the fetcher will extract outlinks
  and follow until the desired depth is reached. A value of 1 means all generated pages are fetched and their first degree
  outlinks are fetched and parsed too. Be careful, this feature is in itself agnostic of the state of the CrawlDB and does not
  know about already fetched pages. A setting larger than 2 will most likely fetch home pages twice in the same fetch cycle.
  It is highly recommended to set db.ignore.external.links to true to restrict the outlink follower to URL's within the same
  domain. When disabled (false) the feature is likely to follow duplicates even when depth=1.
  A value of -1 of 0 disables this feature.
  </description>
</property>

<property>
  <name>fetcher.follow.outlinks.num.links</name>
  <value>500</value>
  <description>(EXPERT)The number of outlinks to follow when fetcher.follow.outlinks.depth is enabled. Be careful, this can multiply
  the total number of pages to fetch. This works with fetcher.follow.outlinks.depth.divisor, by default settings the followed outlinks
  at depth 1 is 8, not 4.
  </description>
</property>

<property>
  <name>fetcher.follow.outlinks.ignore.external</name>
  <value>false</value>
  <description>Whether to ignore or follow external links. Set db.ignore.external.links to false and this to true to store outlinks
  in the output but not follow them. If db.ignore.external.links is true this directive is ignored.
  </description>
</property>

<property>
  <name>fetcher.store.robotstxt</name>
  <value>true</value> <!-- Changed from false -->
  <description>If true (and fetcher.store.content is also true),
  fetcher will store the robots.txt response content and status for
  debugging or archival purposes. The robots.txt is added to the
  content/ folder of the fetched segment.
  </description>
</property>

<property>
  <name>plugin.includes</name>
  <value>protocol-http|protocol-httpclient|urlfilter-(regex)|parse-(html|tika)|index-(basic|more)|indexer-solr|urlnormalizer-(pass|regex|basic)</value>
  <description>Regular expression naming plugin directory names to
  include.  Any plugin not matching this expression is excluded.
  In any case you need at least include the nutch-extensionpoints plugin. By
  default Nutch includes crawling just HTML and plain text via HTTP,
  and basic indexing and search plugins. In order to use HTTPS please enable 
  protocol-httpclient, but be aware of possible intermittent problems with the 
  underlying commons-httpclient library. Set parsefilter-naivebayes for classification based focused crawler.
  </description>
</property>

<!-- parser properties -->

<property>
  <name>parser.html.outlinks.ignore_tags</name>
  <value>script</value>
  <description>Comma separated list of HTML tags, from which outlinks
  shouldn't be extracted. Nutch takes links from: a, area, form, frame,
  iframe, script, link, img. If you add any of those tags here, it
  won't be taken. Default is empty list. Probably reasonable value
  for most people would be "img,script,link".</description>
</property>

<property>
  <name>parse.filter.urls</name>
  <value>true</value>
  <description>Whether the parser will filter URLs (with the configured URL filters).</description>
</property>

<property>
  <name>parser.timeout</name>
  <value>100</value>
  <description>Timeout in seconds for the parsing of a document, otherwise treats it as an exception and
  moves on the the following documents. This parameter is applied to any Parser implementation.
  Set to -1 to deactivate, bearing in mind that this could cause
  the parsing to crash because of a very long or corrupted document.
  </description>
</property>

<property>
  <name>parser.skip.truncated</name>
  <value>false</value> <!-- Changed from true -->
  <description>Boolean value for whether we should skip parsing for truncated documents. By default this
  property is activated due to extremely high levels of CPU which parsing can sometimes take.
  </description>
</property>

<property>
  <name>metatags.names</name>
  <value>*</value> <!-- Changed from description,keywords  -->
  <description> Names of the metatags to extract, separated by ','.
  Use '*' to extract all metatags. Prefixes the names with 'metatag.'
  in the parse-metadata. For instance to index description and keywords,
  you need to activate the plugin index-metadata and set the value of the
  parameter 'index.parse.md' to 'metatag.description,metatag.keywords'.
  </description>
</property>

<property>
  <name>headings</name>
  <value>h1,h2,h3,h4,h5,h6</value>
  <description>Comma separated list of headings to retrieve from the document</description>
</property>

<property>
  <name>solr.server.type</name>
  <value>http</value>
  <description>
    Specifies the SolrServer implementation to use. This is a string value
    of one of the following 'cloud', 'concurrent', 'http' or 'lb'.
    The values represent CloudSolrServer, ConcurrentUpdateSolrServer,
    HttpSolrServer or LBHttpSolrServer respectively.
  </description>
</property>

<property>
  <name>solr.server.url</name>
  <value>http://login1.wrangler.tacc.utexas.edu:8984/solr/sparkler</value>
  <description>
      Defines the Solr URL into which data should be indexed using the
      indexer-solr plugin.
  </description>
</property>

<property>
  <name>solr.zookeeper.url</name>
  <value>http://localhost:9983/</value>
  <description>
      Defines the Zookeeper URL which is an essential setting to be used
      when using SolrCloud. This should be a fully qualified URL similar to
      the property provided within 'solr.server.url' above.
  </description>
</property>

<property>
  <name>solr.mapping.file</name>
  <value>solrindex-mapping.xml</value>
  <description>
  Defines the name of the file that will be used in the mapping of internal
  Nutch field names to solr index fields as specified in the target Solr schema.
  </description>
</property>

<property>
  <name>solr.commit.size</name>
  <value>50</value>
  <description>
  Defines the number of documents to send to Solr in a single update batch.
  Decrease when handling very large documents to prevent Nutch from running
  out of memory. NOTE: It does not explicitly trigger a server side commit.
  </description>
</property>

<property>
  <name>solr.commit.index</name>
  <value>true</value>
  <description>
  When closing the indexer, trigger a commit to the Solr server.
  </description>
</property>

<property>
  <name>moreIndexingFilter.indexMimeTypeParts</name>
  <value>false</value>
  <description>Determines whether the index-more plugin will split the mime-type
  in sub parts, this requires the type field to be multi valued. Set to true for backward
  compatibility. False will not split the mime-type.
  </description>
</property>

</configuration>
